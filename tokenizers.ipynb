{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayiYEglO9T8Z"
   },
   "source": [
    "# Tokenizers\n",
    "\n",
    "For this Colab session, we explore the world of Tokenizers\n",
    "\n",
    "You can run this notebook on a free CPU, or locally on your box if you prefer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9zvDGWD5pKp"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyKWKWSw7Iqp"
   },
   "source": [
    "# Sign in to Hugging Face\n",
    "\n",
    "1. If you haven't already done so, create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions\n",
    "\n",
    "**IMPORTANT** when you create your HuggingFace API key, please be sure to select read and write permissions for your key, otherwise you may get problems later.\n",
    "\n",
    "2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n",
    "`HF_TOKEN = your_token`\n",
    "\n",
    "3. Execute the cell below to log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xd7cEDUC6Lkq"
   },
   "outputs": [],
   "source": [
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSiYqPn87msu"
   },
   "source": [
    "# Accessing Llama 3.1 from Meta\n",
    "\n",
    "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
    "\n",
    "Visit their model instructions page in Hugging Face:\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
    "\n",
    "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
    "\n",
    "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole family of models.\n",
    "\n",
    "If the next cell gives you an error, then please check:  \n",
    "1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n",
    "2. Did you set up your API key with full read and write permissions?\n",
    "3. If you visit the Llama3.1 page with the link above, does it show that you have access to the model near the top?\n",
    "\n",
    "I've also set up this troubleshooting colab to try to diagnose any HuggingFace connectivity issues:  \n",
    "https://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swoiXwUb7RoA"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgJTmHNm8Ui4"
   },
   "outputs": [],
   "source": [
    "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHnp79Ig8vPT"
   },
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtptNsDf83RC"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlQT65oz8-aF"
   },
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7LTUIlD9Gdm"
   },
   "outputs": [],
   "source": [
    "# tokenizer.vocab\n",
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kke10gYj_U87"
   },
   "source": [
    "# Instruct variants of models\n",
    "\n",
    "Many models have a variant that has been trained for use in Chats.  \n",
    "These are typically labelled with the word \"Instruct\" at the end.  \n",
    "They have been trained to expect prompts with a particular format that includes system, user and assistant prompts.  \n",
    "\n",
    "There is a utility method `apply_chat_template` that will convert from the messages list format we are familiar with, into the right input prompt for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DJs4UPx9XfE"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKJjVJ9T_GFn"
   },
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "  ]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlmNvvb-AIKt"
   },
   "source": [
    "# Trying new models\n",
    "\n",
    "We will now work with 3 models:\n",
    "\n",
    "Phi3 from Microsoft\n",
    "Qwen2 from Alibaba Cloud\n",
    "Starcoder2 from BigCode (ServiceNow + HuggingFace + NVidia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWUMdt_iAZQZ"
   },
   "outputs": [],
   "source": [
    "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "QWEN2_MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
    "STARCODER2_MODEL_NAME = \"bigcode/starcoder2-3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WM9tU_ZnAbkR"
   },
   "outputs": [],
   "source": [
    "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
    "\n",
    "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
    "print(tokenizer.encode(text))\n",
    "print()\n",
    "tokens = phi3_tokenizer.encode(text)\n",
    "print(phi3_tokenizer.batch_decode(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CrdGSBZAxx9"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print()\n",
    "print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yr16p4HSA2A4"
   },
   "outputs": [],
   "source": [
    "qwen2_tokenizer = AutoTokenizer.from_pretrained(QWEN2_MODEL_NAME)\n",
    "\n",
    "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
    "print(tokenizer.encode(text))\n",
    "print()\n",
    "print(phi3_tokenizer.encode(text))\n",
    "print()\n",
    "print(qwen2_tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQ5wFS1oBdEN"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print()\n",
    "print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print()\n",
    "print(qwen2_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GGe6hzSBkBg"
   },
   "outputs": [],
   "source": [
    "starcoder2_tokenizer = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME, trust_remote_code=True)\n",
    "code = \"\"\"\n",
    "def hello_world(person):\n",
    "  print(\"Hello\", person)\n",
    "\"\"\"\n",
    "tokens = starcoder2_tokenizer.encode(code)\n",
    "for token in tokens:\n",
    "  print(f\"{token}={starcoder2_tokenizer.decode(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4SQfcrcOAFF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmmDkEjG9Rkz"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
